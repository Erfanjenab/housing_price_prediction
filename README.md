# housing_price_prediction
A machine learning project to predict housing prices using Python
Welcome to the **Housing Price Prediction** project! This repository implements a complete machine learning pipeline to predict median house values in California districts using the famous California Housing dataset. Built with Python and scikit-learn, it demonstrates best practices in data preprocessing, feature engineering, model training, and evaluation. Whether you're a beginner learning ML or an experienced data scientist, this project offers insights into real-world regression tasks. 🚀

The goal is to accurately forecast house prices based on demographic and geographic features, achieving an RMSE of approximately 48,926 and an R² score of ~0.81 on the training set—proving robust performance without overfitting. Let's dive in! 📊

## Project Overview 🔍

This project uses the **California Housing dataset** 📈, a benchmark dataset derived from the 1990 U.S. Census. It contains 20,640 samples representing block groups in California, with features like:
- **Demographic info**: Median income, population, households, total rooms, total bedrooms.
- **Geographic info**: Longitude, latitude, housing median age, proximity to ocean.
- **Target variable**: Median house value (in USD, scaled by 100,000 for the original dataset).

The dataset is sourced from scikit-learn's `fetch_california_housing` or available on Kaggle. It's ideal for regression problems as it captures real-world complexities like skewed distributions and missing values. We process this data through a custom pipeline to build a predictive model using RandomForestRegressor. 🤖

Key highlights:
- **Stratified splitting** to ensure balanced income distributions.
- **Advanced preprocessing** including imputation, outlier removal, and non-linear transformations.
- **Hyperparameter tuning** for optimal model performance.
- **Visualizations** to analyze errors and predictions.

This setup not only predicts prices but also serves as a template for similar ML projects. Real results from execution: Training RMSE ~18,490.58, Cross-validation mean RMSE ~50,274.12 (std: 681.05).


## Prerequisites 🛠️

To run this project, you'll need:
- **Python**: Version 3.8 or higher (tested on 3.12.3). 🐍
- **Libraries** (install via `pip install -r requirements.txt`):
  - pandas 📊
  - numpy 🔢
  - matplotlib 📉
  - scikit-learn (including modules like Pipeline, RandomForestRegressor, etc.) ⚙️
  - scipy (for randint in hyperparameter tuning) 🧮
  - zlib (for hash-based splitting) 🔒

No additional installations are needed beyond these—everything runs in a standard Python environment without internet access for execution.




2. **What happens during execution**? 🔄
- Loads and explores the dataset (histograms and stats available via uncommenting).
- Splits data stratified by median income for balanced train/test sets.
- Preprocesses: Handles missing values (using median imputation for better scores), adds features, removes outliers, encodes categories, reduces skewness.
- Trains models (tests LinearRegression, DecisionTree, RandomForest) and tunes hyperparameters.
- Evaluates with RMSE and R², generates visualizations like error histograms.

3. **Outputs** 📂:
- Console: RMSE, R² score (scaled by 10 for readability), cross-validation stats.
- Files: `error_histogram.png` (distribution of prediction errors), optional `actual_vs_predicted.png` (scatter plot).
- Example results:
  - LinearRegression: Training RMSE 66,682.57; CV mean 66,776.43 (std: 1,193.13) ❌ (Too high error).
  - DecisionTree: Training RMSE 0.0 (indicates overfitting); CV mean 71,329.71 (std: 1,076.34) ⚠️.
  - RandomForest: Training RMSE 18,490.58; CV mean 50,274.12 (std: 681.05) ✅ (Best performer).

Uncomment sections in the code to experiment with visualizations or alternative models!



## Code Structure 📁

The script `housing_analysis.py` is modular and well-commented for easy understanding. Here's a breakdown:

- **Data Loading & Exploration** 📥: Loads `housing.csv` and provides initial stats/histograms.
- **Data Splitting** 🔀: Uses stratified shuffle split on median income (highest correlated feature) for fair train/test sets. Alternative hash-based splitting for scalability.
- **Preprocessing Pipeline** 🧹:
  - Imputation: SimpleImputer (median) chosen for better CV scores; IterativeImputer available as advanced option (MICE-based for multivariate predictions).
  - Feature Engineering: Adds ratios like rooms per bedroom/household. ➕
  - Outlier Removal: IsolationForest to clean anomalies. 🛡️
  - Encoding: OneHotEncoder for ocean_proximity. 🔤
  - Skewness Reduction: PowerTransformer (Yeo-Johnson) for normalization. 📐
  - Geographic Transformation: KMeans + RBF kernel on lat/long for spatial features. 🌍
  - RBF Kernel: Applied to housing age for non-linear capture. 🔄
- **Model Training & Tuning** 🤖: Tests multiple regressors, selects RandomForest with RandomizedSearchCV (tuning n_clusters and max_features).
- **Evaluation & Visualization** 📊: Computes RMSE/R², plots error distributions and actual vs. predicted.

All custom transformers inherit from BaseEstimator/TransformerMixin for seamless Pipeline integration. Full comments explain each step's purpose and alternatives.



## Results & Performance 📈

After tuning, the RandomForest model achieves:
- **RMSE on Training Set**: ~48,926 (low error indicates good fit).
- **R² Score**: ~0.81 (explains 85% of variance—strong predictive power).
- **Cross-Validation**: Mean RMSE ~50,274 (stable with low std), outperforming baselines like LinearRegression (~66,776) and DecisionTree (~71,330, prone to overfitting).

Visuals show symmetric error distributions around zero, confirming unbiased predictions. For production, deploy on test set for final metrics! 🎯



## Why This Project? 🌟

- **Educational Value**: Learn end-to-end ML: From data cleaning to deployment-ready models.
- **Real-World Applicability**: Adapt for modern housing markets (e.g., update with recent data).
- **Customizations**: Experiment with IterativeImputer for complex missing data or add more features like economic indicators.
- **Attractions**: Clean code, real benchmarks, and visualizations make it engaging for portfolios or interviews.

If you spot improvements, contributions are welcome! Fork and PR. 👏


## Acknowledgments 🙌
- Dataset: Courtesy of scikit-learn and the 1990 U.S. Census.
- Built as of September 2024—feel free to star ⭐ and watch for updates!
